{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning model to understand fancy abbreviations\n",
    "\n",
    "Recently I bumped into a [question](https://stackoverflow.com/questions/43510778) on Stackoverflow, how to recover phrases from abbreviations, e.g. turn *wtrbtl* into *water bottle*, and *bsktball* into *basketball*. The question had an additional complication: lack of comprehensive list of word. That means, we need an algorithm able to invent new likely words.\n",
    "\n",
    "I was intrigued, and started researching, which algorithms and math lie behind modern spell-checkers. It turned out that a good spell-checker can be made with a n-gram language model, a model of word distortions, and a greedy beam search algorithm. The whole construction is called a [noisy channel](http://web.stanford.edu/~jurafsky/slp3/5.pdf) model. \n",
    "\n",
    "With this knowledge and Python, I wrote a model from scratch. After training on \"The Fellowship of the Ring\" text, it was able to recognize abbreviations of modern sports terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/1a/fo/ar/1afoarhel5lgye2tszzns09ny9u.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell checkers are widely used: from your phone's keyboard to search engines and voice assistants. It's not easy to make a good spell checker, because it has to be really fast and universal (able to correct unseen words) at the same time. That's why there is so much science in spell checkers. This article is aimed to give idea of this scince and just to make fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math behind a spell checker\n",
    "In the noisy channel model, each abbreviation is the result of a random distortion of the original phrase.\n",
    "\n",
    "To recover the original phrase, we need to answer two questions: which original phrases are likely, and which distortions are likely?\n",
    "\n",
    "\n",
    "By the Bayes theorem, $p(phrase|abbreviation) \\sim p(phrase) p(abbreviation|phrase) = p(phrase) \\sum p(distortion|phrase) $. Here $distortion$ is any change of the original $phrase$, which turns it into the observable $abbreviation$. The $\\sim$ symbol means \"proportional\", because LHS is a probability distribution, but RHS is generally not.\n",
    "\n",
    "Both original phrase likelihood and distortion likelihood can be estimated with statistical models. I will use the simplest models - character n-grams(https://en.wikipedia.org/wiki/N-gram). I could use more difficult models (e.g recurrent neural networks), but it doesn't change the principle.\n",
    "\n",
    "With such models, we can reconstruct probable original phrases letter by letter, using a greedy directed search algorithm.\n",
    "\n",
    "\n",
    "### N-gram language model\n",
    "\n",
    "N-gram model looks at the previous n-1 letters and estimates the probability of the next (n'th) letter conditional on them. For example, probability of letter \"g\" appearing after \"bowlin\" sequence would be calculated by 4-gram model as $p(g|bowlin)=p(g|lin)$, because the model ignores all the characters before these 4, for the sake of simplicity. Conditional probabilities, such as this, are determined (\"learned\") on a training corpus of texts. In my example,\n",
    "$$p(g|lin)=\\frac{\\#(ling)}{\\#(lin\\bullet)}=\\frac{\\#(ling)}{\\#(lina)+\\#(linb)+\\#(linc)+...}$$. \n",
    "Here #(ling) is number of occurences of \"ling\" in the training text. $\\#(lin\\bullet)$  is number of all 4-grams in the text, starting with \"lin\".\n",
    "\n",
    "In order to estimate correctly even the rare n-grams, I apply two tricks. First, for each counter I add a positive number $\\delta$. It guarantees that I will not divide by zero. Second, I use not only n-grams (which can occur rarely in the text), but also n-1 grams (more frequent), and so on, down to 1-grams (unconditional probabilities of letters). But I discount lesser-order counters with an $\\alpha$ multiplier. Thus, in fact I calculate $p(g|lin)$ as \n",
    "\n",
    "$$p(g|lin)=\\frac{(\\#(ling)+1) + \\alpha (\\#(ing)+1) + \\alpha^2 (\\#(ng)+1) + \\alpha^3 (\\#(g)+1)}{(\\#(lin\\bullet)+1) + \\alpha (\\#(in\\bullet)+1) + \\alpha^2 (\\#(n\\bullet)+1) + \\alpha^3 (\\#(\\bullet)+1)}$$\n",
    "\n",
    "For those who prefer implementation to theory, there is the Python code for my n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageNgramModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, за какими буквами следуют какие. \n",
    "    Параметры конструктора:\n",
    "    order - порядок (сколько предыдущих букв помнит модель), или n-1\n",
    "    smoothing - величина, добавляемая к каждому счётчику букв для устойчивости\n",
    "    recursive - вес, с которым используется модель на один порядок меньше\n",
    "    Обучаемые параметры:\n",
    "    counter_ - хранилище частот n-грам, в виде словаря счётчиков. \n",
    "    vocabulary_ - множество всех символов, учитываемых моделью\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Оценка числа всех буквосочетаний по тексту \n",
    "        Параметры:\n",
    "        corpus - текстовая строка. \n",
    "        \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "            \n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Оценка частоты всех символов, которые могут следовать за контекстом \n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает: \n",
    "        freq - вектор условных частот букв, в форме pandas.Series\n",
    "        \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "    \n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Сглаженная оценка вероятности всех символов, которые могут следовать за контекстом \n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает: \n",
    "        freq - вектор условных вероятностей букв, в форме pandas.Series  \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "    \n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Оценка логарифма вероятности конкретного продолжения данной фразы. \n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Оценка вероятности конкретного продолжения данной фразы. \n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model of abbreviations\n",
    "We needed the language model to understand which original phrases are probable. We need the model of abbreviations (or \"distortions\") to understand, how the original phrases usually change.\n",
    "\n",
    "I will assume that the only possible distortion is inclusion of some characters (including whitespace) from the phrase. The model may be modified to take into account other distortion types, e.g. replacements and permutations of characters.\n",
    "\n",
    "I don't have a large sample to train a complex model on it. Therefore, for distortions I will use 1-grams. It means, the model will just remember for each character the probability of its exclusion from abbreviation. But I still code it as a general n-gram model, just in case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MissingLetterModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, какие буквы обычно исключаются из сокращений \n",
    "    Параметры:\n",
    "    order - порядок, или n+1\n",
    "    smoothing_missed - число, прибавляемое к счётчику пропущенных символов\n",
    "    smoothing_total - число, прибавляемое к счётчику всех символов\n",
    "    \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "    \n",
    "    def fit(self, sentence_pairs):\n",
    "        \"\"\" Оценка частоты сокращения символов на основе обучающих примеров \n",
    "        Параметры:\n",
    "        sentence_pairs - список пар (исходная фраза, сокращение)\n",
    "        В сокращении все пропущенные символы заменены на дефисы. \n",
    "        \"\"\"\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1 \n",
    "    \n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Оценка вероятности того, что символ last_letter пропущен после символов context\"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "    \n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка логарифма вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется. \n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token != '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется. \n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy examples\n",
    "\n",
    "Train the bigram language model on just one example and see suggested continutions for \"bra\". \"B\" is the most probable (it goes after \"a\" in most cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.181777\n",
      "a    0.091297\n",
      "b    0.272529\n",
      "c    0.181686\n",
      "d    0.181686\n",
      "r    0.091025\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lang_model = LanguageNgramModel(1)\n",
    "lang_model.fit(' abracadabra ')\n",
    "print(lang_model.predict_proba(' bra'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the distortion model on one (word, distortion) sample. Probability of abbreviating \"a\" away is higher than \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.7166666666666667, 'b': 0.09999999999999999, 'c': 0.15}\n"
     ]
    }
   ],
   "source": [
    "missed_model = MissingLetterModel(0)\n",
    "missed_model.fit([('abracadabra', 'abr-c-d-br-')]) \n",
    "\n",
    "print({letter: missed_model.predict_proba('abr', letter) for letter in 'abc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated probability of \"abra\" abbreviated as \"abr-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.164475\n"
     ]
    }
   ],
   "source": [
    "print(missed_model.single_proba('', 'abra', 'abr-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.313637641589875"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(27) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search for the most probable phrase\n",
    "\n",
    "Having models of language and distortions, theoretically we can estimate likelihood of any original phrase. But for this we need to loop over *all* the possible (original phrase, distortion) pairs. There are just too many of them: e.g. with 27 character alphabet there are $27^{10}$ possible 10-letter phrases. We need a smarter algorithm to avoid this near-infinite looping.\n",
    "\n",
    "I will exploit the fact that the models are single-character-based, and will construct the phrase letter by letter. I wil make a <a href=\"https://en.wikipedia.org/wiki/Heap_(data_structure)\">heap</a> of incomplete candidate phrases, and evaluate likelihood of each. The best candidate will be extended with multiple possible one-letter continuations, and added to the heap. To cut the number of options, I will save only the \"good enough\" candidates. The complete candidates will be set aside, to be returned as an solution in the end. The pricedure will be repeated unless either the heap or the maximum number of iterations runs out.\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/km/1s/61/km1s617dmpsswij9ravmbzjsb8m.jpeg\" width=\"300\"/>\n",
    "\n",
    "The quality of candidates will be evaluated as log-probability of the abreviation, given that the original phrase begins with the candidate and ends (because the candidate is incomplete) as the abbreviation itself. To manage the search, I introduce two parameters: \"optimism\" and \"freedom\". \"Optimism\" evaluates, how the likelihood will improve when the candidate completes. It makes sense to set \"optimism\" between 0 and 1; the closer it is to 1, the faster the algorithm will try to add new characters. \"Freedom\" is the allowable loss of quality in comparison to the curent best candidate. The higher the \"freedom\", the more options would be included, and the slower the algorithm would be. If the \"freedom\" is too low, the heap may deplete before any reasonable phrase is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.929663174828117, '  ', 'brac ', ' ', 3.7800651217336947), (5.0428796453387541, ' a', 'brac ', 'a', 3.4572571306016755), (8.0948719475345303, ' b', 'brac ', 'b', 3.8466616057719989), (7.6238078617051874, ' c', 'brac ', 'c', 3.7800651217336947), (7.6238078617051874, ' d', 'brac ', 'd', 3.7800651217336947), (8.0948719475345303, ' r', 'brac ', 'r', 3.8466616057719989), (4.8582382617757647, ' b', 'rac ', '', 2.8072524973494524)]\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    \"\"\" Генерация вариантов расшифровки аббревиатуры (вспомогательная функция)\n",
    "    Параметры:\n",
    "    prefix_proba - правдоподобие расшифрованной части аббревиатуры\n",
    "    prefix - расшифрованная часть аббревиатуры\n",
    "    suffix - не расшифрованная часть аббревиатуры\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    cache - хранилище оценок качества концов слова\n",
    "    Возвращает: список опций в форме (оценка качества, расшифрованная часть, \n",
    "        не расшифрованная часть, новая буква, оценка качества не расшифрованной части)\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # тут мы считаем, что буква была пропущена\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # тут мы считаем, что пропущенной буквы не было\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options\n",
    "\n",
    "print(generate_options(0, ' ', 'brac ', lang_model, missed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function explores the graph on noisy channel in the best-first manner, until it runs out of attempts or out of optimistic nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=3.0, max_attempts=10000, optimism=0.9, verbose=False):\n",
    "    \"\"\" Подбор фраз, аббревиатурой которых может быть word \n",
    "    Параметры:\n",
    "    word - аббревиатура\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    freedom - возможный зазор по оценке логарифма правдоподобия кандидатов\n",
    "    max_attempts - число итераций\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    verbose - печатать ли наилучших текущих кандидатов в ходе исполнения функции\n",
    "    Возвращает: словарик с ключами - расшифровками \n",
    "        и значениями - минус логарифмом правдоподобия расшифровок. \n",
    "        Чем меньше значение, тем правдоподобнее расшифровка. \n",
    "    \"\"\"\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # добавляем в кучу пустое начало\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # добавляем в кандидаты расшифровку по умолчанию - без пропущенных букв\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        print('baseline score is', best_logprob)\n",
    "    # готовим хранилище вероятностей конфов слов\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # слово расшифровано до конца\n",
    "            # если оно достаточно хорошее, добавим его в кандидаты\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # обновим наилучшую оценку правдоподобия\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply our algorithm to suggest deciphering of \"brc\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score is 7.68318306228\n",
      "(6.9148647560475442, ' ', 'brc ', '', 6.9148647560475442)\n",
      "(6.755450684372974, ' b', 'rc ', '', 4.7044649199466617)\n",
      "(5.8249119494605051, ' br', 'c ', '', 2.6863637325526679)\n",
      "(7.088440394887126, ' brc', ' ', '', 1.7075575253192956)\n",
      "(7.1392598304831516, ' bra', 'c ', 'a', 2.6863637325526679)\n",
      "(7.6831830622750497, ' brc ', '', '', -0.0)\n",
      "(8.0284469273601662, ' brac', ' ', '', 1.7075575253192956)\n",
      "(8.3621576081202385, ' a', 'brc ', 'a', 6.776535093383159)\n",
      "(7.6954572168460142, ' ab', 'rc ', '', 4.7044649199466617)\n",
      "(6.7649184819335453, ' abr', 'c ', '', 2.6863637325526679)\n",
      "(8.0284469273601662, ' abrc', ' ', '', 1.7075575253192956)\n",
      "(8.0792663629561936, ' abra', 'c ', 'a', 2.6863637325526679)\n",
      "(8.6231895947480908, ' abrc ', '', '', -0.0)\n",
      "(8.6231895947480908, ' brac ', '', '', -0.0)\n",
      "(8.6740629096242063, ' brca', ' ', 'a', 1.7075575253192956)\n",
      "heap size is 0 after 15 iterations\n",
      "{'brc': 7.6831830622750497, 'abrc': 8.6231895947480908, 'brac': 8.6231895947480908}\n"
     ]
    }
   ],
   "source": [
    "result = noisy_channel('brc', lang_model, missed_model, verbose=True, freedom=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on hobbits\n",
    "\n",
    "To really test the algorithm, we need a good language model. I was wondering, how well a model could decipher the abbreviations, if it had been trained on a deliberately limited corpus - one book on an unusial topicl. The first such book that got into my hand was \"The Lord Of The Rings: The Fellowship of the Ring\". Well, let's see how well the hobbit language can help to decipher the modern sports term.s\n",
    "\n",
    "But first we need to train the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' abcdefghijklmnopqrstuvwxyz'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# считываем текст\n",
    "with open('Fellowship_of_the_Ring.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "# оставляем только буквы и пробелы в тексте\n",
    "text2 = re.sub(r'[^a-z ]+', '', text.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters)) # ' abcdefghijklmnopqrstuvwxyz'\n",
    "# готовим обучающую выборку для модели опечаток:\n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # тут считаем все буквы пропущенными\n",
    "    + [(all_letters, all_letters)] * 10 # тут считаем все буквы НЕ пропущенными\n",
    "    + [('aeiouy', '------')] * 30 # тут считаем пропущенными только гласные\n",
    ")\n",
    "# обучаем обе модели\n",
    "big_lang_m = LanguageNgramModel(order=4, smoothing=0.001, recursive=0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7674418604651163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33/43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the abbreviation model, I manually created a short corpus, where 25% of consonants and 75% of vowels are abbreviated away.\n",
    "\n",
    "I chose the 5-gram language model, after comparing average likelihood of different models on the \"test set\" (end of the book). It seems that the quality of character probability prediction grows with model order. But I didn't go beyound order of 5, because large order means slow training and application of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -13858.8600648\n",
      "1 -11608.8867664\n",
      "2 -9235.21749986\n",
      "3 -7461.78935696\n",
      "4 -6597.9544372\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 0.001, 0.01)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, I applied the algorithm to different contractions. To begin with, I asked \"sm\", meaning \"Sam\". The model recognized him easily, and added other options (although with higher score, and thus less probable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sam': 7.3438449620080997,\n",
       " 'same': 9.5091694602417469,\n",
       " 'some': 7.6890573935288824}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('sm', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Frodo\" is also deciphered from \"frd\" without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frodo': 6.8904938902680888}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('frd', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the \"ring\" too, from \"rng\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ring': 7.6317120419343913}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('rng', big_lang_m, big_err_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running \"wtrbtl\", I tried the first part, \"wtr\". \"Water\" is deciphered perfectly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water': 8.6405279255413898}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('wtr', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"bottle\" the model is less confident. After all, battles appear more frequently in \"The Lord of the Rings\", than bottles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battle': 12.620490427990008,\n",
       " 'bottle': 13.3327872548629,\n",
       " 'but all': 14.66815480120338,\n",
       " 'but ill': 15.387630853411283}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('btl', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in some contexts, this is exactly what is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battle horse': 25.194823785457018, 'battle horses': 27.40528952535044}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('batlhrse', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"wtrbtl\" the model proposed multiple options, but \"water bottle\" the second among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water battle': 23.76999162985074,\n",
       " 'water bottle': 23.962598992336815,\n",
       " 'water but all': 24.445047133561353,\n",
       " 'water but ill': 25.164523185769259,\n",
       " 'water but lay': 25.601336188357113,\n",
       " 'water but lie': 26.668305553728047}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('wtrbtl', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Basketball\", never seen before, was recognized almost correctly, because the word \"basket\" has occured in the training text. But I had to extend the width of the search beam from 3 to 5, to discover this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bsktball': 33.193085889457429, 'basket ball': 33.985227947093364}\n"
     ]
    }
   ],
   "source": [
    "print(noisy_channel('bsktball', big_lang_m, big_err_m, freedom=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"ball\" has never occured in the training text, so the model failed to recognize \"bowling ball\" in the \"bwlingbl\". But it proposed \"bewilling Bilbo\", \"bowling blow\", and several other alternatives. The word \"bowling\" has also never occured in \"The Lord of the Rings\", but the model somehow managed to reconstruct it with its common understanding of English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bwling blue': 31.318936077746862, 'bwling bilbo': 30.695249686758611, 'bwling ble': 34.490254059547475, 'bwling black': 31.980325659562851, 'bwling blow': 33.15061216480305, 'bewilling blue': 30.937989778499748, 'bewilling bilbo': 30.314303387511497, 'bewilling ble': 34.109307760300361, 'bewilling black': 31.599379360315737, 'bewilling blin': 34.685939493896406, 'bewilling blow': 32.769665865555929, 'bewilling bill': 32.156071732628014, 'bewilling below': 32.195518180732158, 'bwling bill': 32.537018031875135, 'bewilling belia': 32.550377929021479, 'bwling below': 32.576464479979279, 'bwling belia': 32.931324228268608, 'bwling belt': 33.203704016765826, 'bwling bling': 33.393527121566656, 'bwling bell': 34.180762531759534, 'bowling blue': 30.676613106535022, 'bowling bilbo': 30.052926715546771, 'bowling ble': 33.847931088335635, 'bowling black': 31.338002688351011, 'bowling blin': 34.42456282193168, 'bowling blow': 32.508289193591203, 'bowling bill': 31.894695060663285, 'bowling below': 31.934141508767446, 'bowling bl': 34.983414721126472, 'bowling blad': 34.992926061009179, 'bowling belia': 32.289001257056761, 'bwling blind': 35.000922570770712}\n"
     ]
    }
   ],
   "source": [
    "print(noisy_channel('bwlingbl', big_lang_m, big_err_m, freedom=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of weird texts.\n",
    "In the end, to amuse you, I tried to abbreviate the beginning of \"The Lord of the Rings\". It looks weird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This bok s largly cncernd wth Hbbts, nd frm its pges  readr ma dscver much f thir charctr nd  littl f thir hstr. Furthr nfrmaton will als b fond n the selction from the Red Bok f Wstmarch that hs already ben publishd, ndr th ttle of The Hobbit. Tht stor was dervd from the arlir chpters of the Red Bok, cmpsed by Blbo hmslf, th first Hobbit t bcome famos n the world at large, nd clld b him There and Bck Again, sinc thy tld f his journey into th East and his return: n dvntr whch latr nvolved all the Hobbits n th grat vnts of that Ag that re hr rlatd.\n"
     ]
    }
   ],
   "source": [
    "part = text[10502:11149]\n",
    "result = ''\n",
    "for i, letter in enumerate(part):\n",
    "    if np.random.rand() * 0.5 < big_err_m.single_proba(part[0:i], letter):\n",
    "        result += letter\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model may be used to generate a completely new text. It has some Tolkien style, but completely no sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frodo would me but them but his slipped in he see said pippin silent the names for follow as days are or the hobbits rever any forward spoke ened with and many when idle off they hand we cried plunged they lit a simply attack struggled itself it for in a what it was barrow the will the ears what all grow.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "text = \"Frodo\"\n",
    "for i in range(300):\n",
    "    proba = big_lang_m.predict_proba(text)\n",
    "    text += np.random.choice(proba.index, size=1, p=proba)[0]\n",
    "print(text+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of meaningful texts from scatch is still beyound the reach of data science. For it requires real artificial intelligence, able to understand the complex storyline that took place in the Middle-earth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "Natural language processing is a complex mixture of science, technology, and magic. Even linguistic scientists cannot fully understand the laws of \n",
    "\n",
    "Обработка естественного языка - вообще-то очень сложная смесь науки, технологии, и магии. Даже учёные-лингвисты не до конца понимают законов, в соответствии с которыми устроена речь. И до тех времён, когда машины смогут в полном смысле слова понимать тексты, ещё очень не скоро. \n",
    "\n",
    "Обработка естественного языка - это ещё и весело. Вооружившись парой статистических моделек, оказывается, можно и распознавать, и порождать неочевидные аббревиатуры. Для тех, кто хочет продолжить мои развлечения, [вот](https://github.com/avidale/weirdMath/blob/master/nlp/abbreviation_spellchecker_russian.ipynb) полный код эксперимента. \n",
    "\n",
    "А вот если заменить n-граммную модель на рекуррентную нейросетку, можно генерировать тексты более высокой степени связности (и даже почти компилируемый код). В ближайшем будущем я попробую сгенерить нейронкой типичную статью в стиле Хабра (который я уже [распарсил](habrahabr.ru/post/346198/)), поэтому подписывайтесь и ждите :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noisy_channel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8ceae572c18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnoisy_channel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'abrvtn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbig_lang_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbig_err_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreedom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'noisy_channel' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
