{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обучть мдль пнмть упртые скрщня\n",
    "\n",
    "Недавно я натолкнулся на [вопрос](https://stackoverflow.com/questions/43510778) на Stackoverflow, как восстанавливать исходные слова из сокращений: например, из *wtrbtl* получать *water bottle*, а из *bsktball* - *basketball*. В вопросе было дополнительное усложнение: полного словаря всех возможных исходных слов нет, т.е. алгоритм должен быть в состоянии придумывать новые слова. \n",
    "\n",
    "Вопрос меня заинтриговал, и я полез разбираться, какие алгоритмы и математика лежат в основе современных опечаточников (spell-checkers). Оказалось, что хороший опечаточник можно собрать из n-граммной языковой модели, модели вероятности искажений слов, и жадного алгоритма поиска по лучу (beam search). Вся конструкция вместе называется модель [зашумлённого канала](http://web.stanford.edu/~jurafsky/slp3/5.pdf) (noisy channel). \n",
    "\n",
    "Вооружившись этими знаниями и Питоном, я за вечер создал с нуля модельку, способную, обучившись на тексте \"Властелина колец\", распознавать сокращения вполне современных спортивных терминов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/1a/fo/ar/1afoarhel5lgye2tszzns09ny9u.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опечаточники используются много где: от клавиатуры вашего телефона до поисковых систем и голосовых помощников. Сделать хороший опечаточник не так просто, ибо он должен быть одновременно быстрым и готовым ко всему (в том числе исправить ошибки в словах, которые до этого никогда не видел). Именно поэтому в опечаточниках используется столько науки. Цель статьи - дать представление об этой науке и просто развлечься. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Математика внутри опечаточника\n",
    "Модель зашумлённого канала рассматривает каждую аббревиатуру как результат случайного искажения исходной фразы.\n",
    "\n",
    "Чтобы восстановить оригинальное словосочетание, надо ответить на два вопроса: какие исходные слова вероятны, и какие искажения фраз вероятны? \n",
    "\n",
    "По теореме Байеса, $p(phrase|abbreviation) \\sim p(phrase) p(abbreviation|phrase) = p(phrase) \\sum p(distortion|phrase) $, где $distortion$ - это любое такое изменение исходной $phrase$, которое делает из неё наблюдаемую $abbreviation$. Символ $\\sim$ обозначает прямую пропорциональность. \n",
    "\n",
    "И вероятность всевозможнных фраз, и вероятность трансформаций могут быть оценены с помощью статистических моделей. Я буду использовать простейший вид моделей -  буквенные [n-граммы](https://en.wikipedia.org/wiki/N-gram). Можно было бы применять и более сложные модели (например, рекуррентные нейросети), принципиально это ничего не меняет. \n",
    "\n",
    "Имея такие модели, можно подобрать наиболее правдоподобную исходную фразу буква за буквой, используя алгоритм жадного направленного поиска, beam search.  \n",
    "\n",
    "### N-граммная языковая модель\n",
    "\n",
    "n-граммная модель по предыдущим n-1 буквам фразы определяет вероятность следующей, n-той буквы (и игнорирует все буквы, которые были перед ними). Например, вероятность буквы $g$ после последовательности $bowlin$ будет вычислена 4-граммной моделью как $p(g|bowlin)=p(g|lin)$, т.к. все буквы, предшествующие этим четырём, модель для простоты игнорирует. Такие условные вероятности определяются (\"выучиваются\") по обучающему корпусу текстов. Продолжая пример, $p(g|lin)=\\frac{\\#(ling)}{\\#(lin\\bullet)}=\\frac{\\#(ling)}{\\#(lina)+\\#(linb)+\\#(linc)+...}$, где $\\#(ling)$ - это количество сочетаний $ling$ в обучающем корпусе, а $\\#(lin\\bullet)$ - количество всех 4-грамм в тексте, начинающихся на $lin$.\n",
    "\n",
    "Чтобы оценивать вероятности даже редких n-грамм более адекватно, я применил две уловки. Во-первых, к каждому счётчику я прибавляю число $\\delta$ - это гарантирует, что я не буду делить на ноль. Во-вторых, для подсчёта я использую не только $n$-граммы (которых может быть мало в тексте), но и $n-1$-граммы (они встречаются чаще), и так далее, вплоть до самой простой модели, которая вообще не смотрит на контекст. Но счётчики меньших порядков используются с весами, убывающими в геометрической прогрессии с темпом $\\alpha$. Поэтому на самом деле вероятность $p(g|lin)$ я вычисляю как\n",
    "$$p(g|lin)=\\frac{(\\#(ling)+1) + \\alpha (\\#(ing)+1) + \\alpha^2 (\\#(ng)+1) + \\alpha^3 (\\#(g)+1)}{(\\#(lin\\bullet)+1) + \\alpha (\\#(in\\bullet)+1) + \\alpha^2 (\\#(n\\bullet)+1) + \\alpha^3 (\\#(\\bullet)+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Впрочем, довольно теории! Посмотрим лучше, как выглядит код модели на языке Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageNgramModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, за какими буквами следуют какие. \n",
    "    Параметры конструктора:\n",
    "    order - порядок (сколько предыдущих букв помнит модель), или n-1\n",
    "    smoothing - величина, добавляемая к каждому счётчику букв для устойчивости\n",
    "    recursive - вес, с которым используется модель на один порядок меньше\n",
    "    Обучаемые параметры:\n",
    "    counter_ - хранилище частот n-грам, в виде словаря счётчиков. \n",
    "    vocabulary_ - множество всех символов, учитываемых моделью\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
    "        self.order = order\n",
    "        self.smoothing = smoothing\n",
    "        self.recursive = recursive\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\" Оценка числа всех буквосочетаний по тексту \n",
    "        Параметры:\n",
    "        corpus - текстовая строка. \n",
    "        \"\"\"\n",
    "        self.counter_ = defaultdict(lambda: Counter())\n",
    "        self.vocabulary_ = set()\n",
    "        for i, token in enumerate(corpus[self.order:]):\n",
    "            context = corpus[i:(i+self.order)]\n",
    "            self.counter_[context][token] += 1\n",
    "            self.vocabulary_.add(token)\n",
    "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
    "            self.child_.fit(corpus)\n",
    "            \n",
    "    def get_counts(self, context):\n",
    "        \"\"\" Оценка частоты всех символов, которые могут следовать за контекстом \n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает: \n",
    "        freq - вектор условных частот букв, в форме pandas.Series\n",
    "        \"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        freq_dict = self.counter_[local]\n",
    "        freq = pd.Series(index=self.vocabulary_)\n",
    "        for i, token in enumerate(self.vocabulary_):\n",
    "            freq[token] = freq_dict[token] + self.smoothing\n",
    "        if self.recursive > 0 and self.order > 0:\n",
    "            child_freq = self.child_.get_counts(context) * self.recursive\n",
    "            freq += child_freq\n",
    "        return freq\n",
    "    \n",
    "    def predict_proba(self, context):\n",
    "        \"\"\" Сглаженная оценка вероятности всех символов, которые могут следовать за контекстом \n",
    "        Параметры:\n",
    "        context - текстовая строка (учиываются только последние self.order символов)\n",
    "        Возвращает: \n",
    "        freq - вектор условных вероятностей букв, в форме pandas.Series  \"\"\"\n",
    "        counts = self.get_counts(context)\n",
    "        return counts / counts.sum()\n",
    "    \n",
    "    def single_log_proba(self, context, continuation):\n",
    "        \"\"\" Оценка логарифма вероятности конкретного продолжения данной фразы. \n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        for token in continuation:\n",
    "            result += np.log(self.predict_proba(context)[token])\n",
    "            context += token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation):\n",
    "        \"\"\" Оценка вероятности конкретного продолжения данной фразы. \n",
    "        Параметры:\n",
    "        context - текстовая строка, известное начало фразы\n",
    "        continuation - текстовая строка, гипотетическое продолжение фразы\n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель \"опечаток\"\n",
    "Языковая модель была нужна, чтобы понять, какими обычно бывают исходные фразы. Модель искажений нужна, чтобы понять, как обычно исходные фразы искажаются. \n",
    "\n",
    "Я буду предполагать, что единственное возможное искажение - это вычёркивание из фразы некоторых символов. При желании, модель можно модифицировать, чтобы она учитывала и другие искажения, например, замены и перестановки букв. \n",
    "\n",
    "У меня нет большой выборки, на которой можно обучать сложную модель. Поэтому для оценки вероятности искажений я буду использовать 1-граммы. То есть моя модель будет просто запоминать для каждого символа, с какой вероятностью он вычёркивается из сокращения. Впрочем, закодирую я её на всякий случай в общем виде, как n-граммную модель. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MissingLetterModel:\n",
    "    \"\"\" Модель запоминает и предсказывает, какие буквы обычно исключаются из сокращений \n",
    "    Параметры:\n",
    "    order - порядок, или n+1\n",
    "    smoothing_missed - число, прибавляемое к счётчику пропущенных символов\n",
    "    smoothing_total - число, прибавляемое к счётчику всех символов\n",
    "    \"\"\"\n",
    "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
    "        self.order = order\n",
    "        self.smoothing_missed = smoothing_missed\n",
    "        self.smoothing_total = smoothing_total\n",
    "    \n",
    "    def fit(self, sentence_pairs):\n",
    "        \"\"\" Оценка частоты сокращения символов на основе обучающих примеров \n",
    "        Параметры:\n",
    "        sentence_pairs - список пар (исходная фраза, сокращение)\n",
    "        В сокращении все пропущенные символы заменены на дефисы. \n",
    "        \"\"\"\n",
    "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
    "        self.total_counter_ = defaultdict(lambda: Counter())\n",
    "        for (original, observed) in sentence_pairs:\n",
    "            for i, (original_letter, observed_letter) in enumerate(zip(original[self.order:], observed[self.order:])):\n",
    "                context = original[i:(i+self.order)]\n",
    "                if observed_letter == '-':\n",
    "                    self.missed_counter_[context][original_letter] += 1\n",
    "                self.total_counter_[context][original_letter] += 1 \n",
    "    \n",
    "    def predict_proba(self, context, last_letter):\n",
    "        \"\"\" Оценка вероятности того, что символ last_letter пропущен после символов context\"\"\"\n",
    "        if self.order:\n",
    "            local = context[-self.order:]\n",
    "        else:\n",
    "            local = ''\n",
    "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
    "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
    "        return missed_freq / total_freq\n",
    "    \n",
    "    def single_log_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка логарифма вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется. \n",
    "        \"\"\"\n",
    "        if not actual:\n",
    "            actual = continuation\n",
    "        result = 0.0\n",
    "        for orig_token, act_token in zip(continuation, actual):\n",
    "            pp = self.predict_proba(context, orig_token)\n",
    "            if act_token != '-':\n",
    "                pp = 1 - pp\n",
    "            result += np.log(pp)\n",
    "            context += orig_token\n",
    "        return result\n",
    "    \n",
    "    def single_proba(self, context, continuation, actual=None):\n",
    "        \"\"\" Оценка вероятности того, после фразы context фраза continuation трансформируется в actual\n",
    "        Если actual не указана, предполагается, что continuation не изменяется. \n",
    "        \"\"\"\n",
    "        return np.exp(self.single_log_proba(context, continuation, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример работы моделей\n",
    "\n",
    "Чтобы понять, как работают модели языка и сокращений, посмотрим на простой пример. Языковая модель обучим на одном слове и посмотрим, как она предсказывает продолжение слова \"bra\". С её точки зрения, \"b\" - самое правдоподобное продолжение (потому что эта буква чаще всего идёт после \"a\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.181777\n",
      "a    0.091297\n",
      "b    0.272529\n",
      "c    0.181686\n",
      "d    0.181686\n",
      "r    0.091025\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lang_model = LanguageNgramModel(1)\n",
    "lang_model.fit(' abracadabra ')\n",
    "print(lang_model.predict_proba(' bra'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сокращений обучим тоже на одном примере (слово, сокращение). В этом примере мы сокращали только букву \"а\", поэтому для неё вероятность сокращения модель оценивает высоко, а для буквы \"b\" - ниже. Вероятность пропуска буквы \"c\" модель оценила выше, чем \"b\", потому что \"с\" она реже встречала в обучающих примерах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.7166666666666667, 'b': 0.09999999999999999, 'c': 0.15}\n"
     ]
    }
   ],
   "source": [
    "missed_model = MissingLetterModel(0)\n",
    "missed_model.fit([('abracadabra', 'abr-c-d-br-')]) \n",
    "\n",
    "print({letter: missed_model.predict_proba('abr', letter) for letter in 'abc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так модель сокращений оценивает вероятность того, что \"abra\" будет сокращена как \"abr-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.164475\n"
     ]
    }
   ],
   "source": [
    "print(missed_model.single_proba('', 'abra', 'abr-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.313637641589875"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(27) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадный поиск наиболее правдоподобной фразы\n",
    "\n",
    "Имея модели языка и искажений, теоретически можно оценить вероятность любой исходной фразы. Для этого нужно всего лишь перебрать *все* возможные пары (исходная фраза, трансформация) и оценить правдоподобие каждой. Но их слишком много: например, в алфавите из 27 символов возможны $27^{10}$ фраз из 10 букв; перебрать их все - нереально. Нужен более хитрый алгоритм. \n",
    "\n",
    "Мы воспользуемся тем, что обе модели - однобуквенные, и будем подбирать буквы исходной фразы одну за другой. Для этого мы заведём <a href=\"https://ru.wikipedia.org/wiki/Куча_(память)\">кучу</a> неполных фраз-кандидатов, и для каждой оценим, насколько она соответствует аббревиатуре. Наилучшего кандидата мы дополним всеми разнообразными буквами-продолжениями и гипотезами о том, были ли они пропущены, и добавим в кучу. Чтобы не создавать слишком много вариантов, будем брать только \"достаточно хорошие\" продолжения.  Тех кандидатов, которые уже могут быть полноценными исходными фразами, мы отложим в сторону, и в конце выдадим в качестве ответа. Процедуру будем повторять, пока не закончится куча либо максимальное число итераций.\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/km/1s/61/km1s617dmpsswij9ravmbzjsb8m.jpeg\" width=\"300\"/>\n",
    "\n",
    "Как меру качества кандидатов мы будем использовать оценку логарифма вероятности имеющейся аббревиатуры при условии того, что исходная фраза начинается, как этот кандидат, а заканчивается, как сама аббревиатура. Для управления поиском введём два параметра - *optimism* и *freedom*. *Optimism* оценивает, насколько улучшится расшифровка, когда мы дойдём до конца аббревиатуры. Этот коэффициент имеет смысл выбирать между 0 и 1, и чем ближе он к 1, тем быстрее алгоритм будет пытаться добавить новые символы в расшифровку. *Freedom* - это то, насколько качество кандидатов может быть хуже наилучшего текущего варианта, и при увеличении этого параметра алгоритм рассматривает больше различных вариантов. Если установить очень высокое значение *freedom*, алгоритм будет перебирать все варианты (коих бесконечность), и работать очень долго, но при маленьких значениях *freedom* куча может закончиться до того, как будет найден хоть один адекватный вариант. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.929663174828117, '  ', 'brac ', ' ', 3.7800651217336947), (5.0428796453387541, ' a', 'brac ', 'a', 3.4572571306016755), (8.0948719475345303, ' b', 'brac ', 'b', 3.8466616057719989), (7.6238078617051874, ' c', 'brac ', 'c', 3.7800651217336947), (7.6238078617051874, ' d', 'brac ', 'd', 3.7800651217336947), (8.0948719475345303, ' r', 'brac ', 'r', 3.8466616057719989), (4.8582382617757647, ' b', 'rac ', '', 2.8072524973494524)]\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "def generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism=0.5, cache=None):\n",
    "    \"\"\" Генерация вариантов расшифровки аббревиатуры (вспомогательная функция)\n",
    "    Параметры:\n",
    "    prefix_proba - правдоподобие расшифрованной части аббревиатуры\n",
    "    prefix - расшифрованная часть аббревиатуры\n",
    "    suffix - не расшифрованная часть аббревиатуры\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    cache - хранилище оценок качества концов слова\n",
    "    Возвращает: список опций в форме (оценка качества, расшифрованная часть, \n",
    "        не расшифрованная часть, новая буква, оценка качества не расшифрованной части)\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for letter in lang_model.vocabulary_ + ['']:\n",
    "        if letter:  # тут мы считаем, что буква была пропущена\n",
    "            next_letter = letter\n",
    "            new_suffix = suffix\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log(missed_model.predict_proba(prefix, letter))\n",
    "        else:  # тут мы считаем, что пропущенной буквы не было\n",
    "            next_letter = suffix[0]\n",
    "            new_suffix = suffix[1:]\n",
    "            new_prefix = prefix + next_letter\n",
    "            proba_missing_state = - np.log((1 - missed_model.predict_proba(prefix, next_letter)))\n",
    "        proba_next_letter = - np.log(lang_model.single_proba(prefix, next_letter))\n",
    "        if cache:\n",
    "            proba_suffix = cache[len(new_suffix)] * optimism\n",
    "        else:\n",
    "            proba_suffix = - np.log(lang_model.single_proba(new_prefix, new_suffix)) * optimism\n",
    "        proba = prefix_proba + proba_next_letter + proba_missing_state + proba_suffix\n",
    "        options.append((proba, new_prefix, new_suffix, letter, proba_suffix))\n",
    "    return options\n",
    "\n",
    "print(generate_options(0, ' ', 'brac ', lang_model, missed_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function explores the graph on noisy channel in the best-first manner, until it runs out of attempts or out of optimistic nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_channel(word, lang_model, missed_model, freedom=3.0, max_attempts=10000, optimism=0.9, verbose=False):\n",
    "    \"\"\" Подбор фраз, аббревиатурой которых может быть word \n",
    "    Параметры:\n",
    "    word - аббревиатура\n",
    "    lang_model - модель языка\n",
    "    missed_model - модель вероятности сокращений\n",
    "    freedom - возможный зазор по оценке логарифма правдоподобия кандидатов\n",
    "    max_attempts - число итераций\n",
    "    optimism - коэффициент, с которым учитывается не объясненный конец слова\n",
    "    verbose - печатать ли наилучших текущих кандидатов в ходе исполнения функции\n",
    "    Возвращает: словарик с ключами - расшифровками \n",
    "        и значениями - минус логарифмом правдоподобия расшифровок. \n",
    "        Чем меньше значение, тем правдоподобнее расшифровка. \n",
    "    \"\"\"\n",
    "    query = word + ' '\n",
    "    prefix = ' '\n",
    "    prefix_proba = 0.0\n",
    "    suffix = query\n",
    "    full_origin_logprob = -lang_model.single_log_proba(prefix, query)\n",
    "    no_missing_logprob = -missed_model.single_log_proba(prefix, query)\n",
    "    best_logprob = full_origin_logprob + no_missing_logprob\n",
    "    # добавляем в кучу пустое начало\n",
    "    heap = [(best_logprob * optimism, prefix, suffix, '', best_logprob * optimism)]\n",
    "    # добавляем в кандидаты расшифровку по умолчанию - без пропущенных букв\n",
    "    candidates = [(best_logprob, prefix + query, '', None, 0.0)]\n",
    "    if verbose:\n",
    "        print('baseline score is', best_logprob)\n",
    "    # готовим хранилище вероятностей конфов слов\n",
    "    cache = {}\n",
    "    for i in range(len(query)+1):\n",
    "        future_suffix = query[:i]\n",
    "        cache[len(future_suffix)] = -lang_model.single_log_proba('', future_suffix) # rough approximation\n",
    "        cache[len(future_suffix)] += -missed_model.single_log_proba('', future_suffix) # at least add missingness\n",
    "    \n",
    "    for i in range(max_attempts):\n",
    "        if not heap:\n",
    "            break\n",
    "        next_best = heappop(heap)\n",
    "        if verbose:\n",
    "            print(next_best)\n",
    "        if next_best[2] == '':  # слово расшифровано до конца\n",
    "            # если оно достаточно хорошее, добавим его в кандидаты\n",
    "            if next_best[0] <= best_logprob + freedom:\n",
    "                candidates.append(next_best)\n",
    "                # обновим наилучшую оценку правдоподобия\n",
    "                if next_best[0] < best_logprob:\n",
    "                    best_logprob = next_best[0]\n",
    "        else: # it is not a leaf - generate more options\n",
    "            prefix_proba = next_best[0] - next_best[4] # all proba estimate minus suffix\n",
    "            prefix = next_best[1]\n",
    "            suffix = next_best[2]\n",
    "            new_options = generate_options(prefix_proba, prefix, suffix, lang_model, missed_model, optimism, cache)\n",
    "            # add only the solution potentioally no worse than the best + freedom\n",
    "            for new_option in new_options: \n",
    "                if new_option[0] < best_logprob + freedom:\n",
    "                    heappush(heap, new_option)\n",
    "    if verbose:\n",
    "        print('heap size is', len(heap), 'after', i, 'iterations')\n",
    "    result = {}\n",
    "    for candidate in candidates:\n",
    "        if candidate[0] <= best_logprob + freedom:\n",
    "            result[candidate[1][1:-1]] = candidate[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим наш алгоритм для поиска возможных расшифровок аббревиатуры \"brc\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score is 7.68318306228\n",
      "(6.9148647560475442, ' ', 'brc ', '', 6.9148647560475442)\n",
      "(6.755450684372974, ' b', 'rc ', '', 4.7044649199466617)\n",
      "(5.8249119494605051, ' br', 'c ', '', 2.6863637325526679)\n",
      "(7.088440394887126, ' brc', ' ', '', 1.7075575253192956)\n",
      "(7.1392598304831516, ' bra', 'c ', 'a', 2.6863637325526679)\n",
      "(7.6831830622750497, ' brc ', '', '', -0.0)\n",
      "(8.0284469273601662, ' brac', ' ', '', 1.7075575253192956)\n",
      "(8.3621576081202385, ' a', 'brc ', 'a', 6.776535093383159)\n",
      "(7.6954572168460142, ' ab', 'rc ', '', 4.7044649199466617)\n",
      "(6.7649184819335453, ' abr', 'c ', '', 2.6863637325526679)\n",
      "(8.0284469273601662, ' abrc', ' ', '', 1.7075575253192956)\n",
      "(8.0792663629561936, ' abra', 'c ', 'a', 2.6863637325526679)\n",
      "(8.6231895947480908, ' abrc ', '', '', -0.0)\n",
      "(8.6231895947480908, ' brac ', '', '', -0.0)\n",
      "(8.6740629096242063, ' brca', ' ', 'a', 1.7075575253192956)\n",
      "heap size is 0 after 15 iterations\n",
      "{'brc': 7.6831830622750497, 'abrc': 8.6231895947480908, 'brac': 8.6231895947480908}\n"
     ]
    }
   ],
   "source": [
    "result = noisy_channel('brc', lang_model, missed_model, verbose=True, freedom=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование на хоббитах\n",
    "\n",
    "По-настоящему протестировать написанные алгоритмы можно только с хорошей языковой моделью. Мне было интересно, как качественно модель сможет расшифровывать аббревиатуры, обучившись на заведомо ограниченном корпусе - одной книге, да ещё и специфической тематики. Первая книга, которая попалась мне под руку - это \"The Lord Of The Rings: The Fellowship of the Ring\". Что ж, посмотрим, как язык хоббитов может помочь расшифровать современные спортивные термины. \n",
    "\n",
    "Для начала - код для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' abcdefghijklmnopqrstuvwxyz'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# считываем текст\n",
    "with open('Fellowship_of_the_Ring.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "# оставляем только буквы и пробелы в тексте\n",
    "text2 = re.sub(r'[^a-z ]+', '', text.lower().replace('\\n', ' '))\n",
    "all_letters = ''.join(list(sorted(list(set(text2)))))\n",
    "print(repr(all_letters)) # ' abcdefghijklmnopqrstuvwxyz'\n",
    "# готовим обучающую выборку для модели опечаток:\n",
    "missing_set =  (\n",
    "    [(all_letters, '-' * len(all_letters))] * 3 # тут считаем все буквы пропущенными\n",
    "    + [(all_letters, all_letters)] * 10 # тут считаем все буквы НЕ пропущенными\n",
    "    + [('aeiouy', '------')] * 30 # тут считаем пропущенными только гласные\n",
    ")\n",
    "# обучаем обе модели\n",
    "big_lang_m = LanguageNgramModel(order=4, smoothing=0.001, recursive=0.01)\n",
    "big_lang_m.fit(text2)\n",
    "big_err_m = MissingLetterModel(order=0, smoothing_missed=0.1)\n",
    "big_err_m.fit(missing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Четырёхграммную модель языка я выбрал, сравнив правдоподобие разных моделей на \"тестовой выборке\" - самом конце книги. Оказалось, что качество предсказания вероятности букв растёт с ростом порядка модели до 4. Ещё больший порядок я не стал использовать, ибо с ростом порядка сильно падает скорость работы модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -13858.8600648\n",
      "1 -11608.8867664\n",
      "2 -9235.21749986\n",
      "3 -7461.78935696\n",
      "4 -6597.9544372\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tmp = LanguageNgramModel(i, 0.001, 0.01)\n",
    "    tmp.fit(text2[0:-5000])\n",
    "    print(i, tmp.single_log_proba(' ', text2[-5000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим наш алгоритм к разным сокращениям. Оказалось, что слова, часто встречающиеся в обучающем тексте, распознаются легко. Чтобы хорошо распознать \"bsktball\" и \"bwlingbl\", пришлось увеличить \"ширину луча\", в котором подбираются слова. Впрочем, вместо \"ball\" модель по понятным причинам предлагает \"bilbo\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sam': 7.3438449620080997,\n",
       " 'same': 9.5091694602417469,\n",
       " 'some': 7.6890573935288824}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('sm', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frodo': 6.8904938902680888}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('frd', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ring': 7.6317120419343913}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('rng', big_lang_m, big_err_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water': 8.6405279255413898}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('wtr', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battle': 12.620490427990008,\n",
       " 'bottle': 13.3327872548629,\n",
       " 'but all': 14.66815480120338,\n",
       " 'but ill': 15.387630853411283}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('btl', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battle horse': 25.194823785457018, 'battle horses': 27.40528952535044}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('batlhrse', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'water battle': 23.76999162985074,\n",
       " 'water bottle': 23.962598992336815,\n",
       " 'water but all': 24.445047133561353,\n",
       " 'water but ill': 25.164523185769259,\n",
       " 'water but lay': 25.601336188357113,\n",
       " 'water but lie': 26.668305553728047}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_channel('wtrbtl', big_lang_m, big_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bsktball': 33.193085889457429, 'basket ball': 33.985227947093364}\n"
     ]
    }
   ],
   "source": [
    "print(noisy_channel('bsktball', big_lang_m, big_err_m, freedom=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bwling blue': 31.318936077746862, 'bwling bilbo': 30.695249686758611, 'bwling ble': 34.490254059547475, 'bwling black': 31.980325659562851, 'bwling blow': 33.15061216480305, 'bewilling blue': 30.937989778499748, 'bewilling bilbo': 30.314303387511497, 'bewilling ble': 34.109307760300361, 'bewilling black': 31.599379360315737, 'bewilling blin': 34.685939493896406, 'bewilling blow': 32.769665865555929, 'bewilling bill': 32.156071732628014, 'bewilling below': 32.195518180732158, 'bwling bill': 32.537018031875135, 'bewilling belia': 32.550377929021479, 'bwling below': 32.576464479979279, 'bwling belia': 32.931324228268608, 'bwling belt': 33.203704016765826, 'bwling bling': 33.393527121566656, 'bwling bell': 34.180762531759534, 'bowling blue': 30.676613106535022, 'bowling bilbo': 30.052926715546771, 'bowling ble': 33.847931088335635, 'bowling black': 31.338002688351011, 'bowling blin': 34.42456282193168, 'bowling blow': 32.508289193591203, 'bowling bill': 31.894695060663285, 'bowling below': 31.934141508767446, 'bowling bl': 34.983414721126472, 'bowling blad': 34.992926061009179, 'bowling belia': 32.289001257056761, 'bwling blind': 35.000922570770712}\n"
     ]
    }
   ],
   "source": [
    "print(noisy_channel('bwlingbl', big_lang_m, big_err_m, freedom=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация упоротых текстов\n",
    "Под конец, чтобы немного развеселить вас, я попробовал превратить в сокращения уже сам кусок текста \"Властелина колец\". Получилось странно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This bok s largly cncernd wth Hbbts, nd frm its pges  readr ma dscver much f thir charctr nd  littl f thir hstr. Furthr nfrmaton will als b fond n the selction from the Red Bok f Wstmarch that hs already ben publishd, ndr th ttle of The Hobbit. Tht stor was dervd from the arlir chpters of the Red Bok, cmpsed by Blbo hmslf, th first Hobbit t bcome famos n the world at large, nd clld b him There and Bck Again, sinc thy tld f his journey into th East and his return: n dvntr whch latr nvolved all the Hobbits n th grat vnts of that Ag that re hr rlatd.\n"
     ]
    }
   ],
   "source": [
    "part = text[10502:11149]\n",
    "result = ''\n",
    "for i, letter in enumerate(part):\n",
    "    if np.random.rand() * 0.5 < big_err_m.single_proba(part[0:i], letter):\n",
    "        result += letter\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковую модель можно использовать, чтобы генерировать тексты. Получилось \"в стиле Толкина\", правда, безо всякого смысла. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frodo would me but them but his slipped in he see said pippin silent the names for follow as days are or the hobbits rever any forward spoke ened with and many when idle off they hand we cried plunged they lit a simply attack struggled itself it for in a what it was barrow the will the ears what all grow.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(20)\n",
    "text = \"Frodo\"\n",
    "for i in range(300):\n",
    "    proba = big_lang_m.predict_proba(text)\n",
    "    text += np.random.choice(proba.index, size=1, p=proba)[0]\n",
    "print(text+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерировать \"с нуля\" связные тексты наука о данных пока так и не научилась. Ибо для этого нужен самый настоящий искусственный интеллект, обладающий пониманием сюжета, разворачивавшегося когда-то в Средиземье. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "Обработка естественного языка - вообще-то очень сложная смесь науки, технологии, и магии. Даже учёные-лингвисты не до конца понимают законов, в соответствии с которыми устроена речь. И до тех времён, когда машины смогут в полном смысле слова понимать тексты, ещё очень не скоро. \n",
    "\n",
    "Обработка естественного языка - это ещё и весело. Вооружившись парой статистических моделек, оказывается, можно и распознавать, и порождать неочевидные аббревиатуры. Для тех, кто хочет продолжить мои развлечения, [вот](https://github.com/avidale/weirdMath/blob/master/nlp/abbreviation_spellchecker_russian.ipynb) полный код эксперимента. \n",
    "\n",
    "А вот если заменить n-граммную модель на рекуррентную нейросетку, можно генерировать тексты более высокой степени связности (и даже почти компилируемый код). В ближайшем будущем я попробую сгенерить нейронкой типичную статью в стиле Хабра (который я уже [распарсил](habrahabr.ru/post/346198/)), поэтому подписывайтесь и ждите :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
